{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies for Computervision (cv2)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# From step 2\n",
    "import os\n",
    "\n",
    "# From step 3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline # allows you to build a ML pipeline\n",
    "from sklearn.preprocessing import StandardScaler # Normalizes your data\n",
    "\n",
    "# Diferent algorithms for diversifing your model training (?)\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# From step 4\n",
    "from sklearn.metrics import accuracy_score # accuracy metrics\n",
    "import pickle\n",
    "\n",
    "# Import dependencies for TEXT_Generation (GPTNEO)\n",
    "# from transformers import pipeline\n",
    "# from transformers import GPTNeoModel, GPTNeoConfig\n",
    "\n",
    "# Import OSC\n",
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server\n",
    "from pythonosc import osc_message_builder\n",
    "from pythonosc import udp_client\n",
    "\n",
    "\n",
    "# Misc dependencies\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "# GPT3 OPENAI\n",
    "import json\n",
    "import openai\n",
    "from gpt import GPT\n",
    "from gpt import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TensorFlow MobileNet-SDD v3 object detection\n",
    "config_file ='./assets/ssd_mobilenet_v3/config/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
    "frozen_model = './assets/ssd_mobilenet_v3/weights/ssd_mobilenet_v3_large_coco_2020_01_14/frozen_inference_graph.pb'\n",
    "\n",
    "# Load the model\n",
    "model = cv2.dnn_DetectionModel(frozen_model, config_file)\n",
    "\n",
    "# Load coco labels\n",
    "classLabels = []\n",
    "file_name = './assets/labels/darknet_coco_names.txt'\n",
    "with open(file_name,'rt') as fpt:\n",
    "    classLabels = fpt.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "# Set up the model\n",
    "model.setInputSize(320, 320) ## size specs from the config file\n",
    "model.setInputScale(1.0/127.5) ## 255/2 = 127.5\n",
    "model.setInputMean((127.5, 127.5, 127.5)) ## mobilenet => [-1,1]\n",
    "\n",
    "#load an image\n",
    "img = cv2.imread('./assets/images/person_with_dog.jpg')\n",
    "\n",
    "# Using the model to detect classes from the index of our labels\n",
    "ClassIndex, confidence, bbox = model.detect(img,confThreshold=0.6)\n",
    "\n",
    "# Gender detection setup\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['man ', 'woman ']\n",
    "\n",
    "#load the network\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ClassIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup models\n",
    "# text generator setup\n",
    "# generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n",
    "\n",
    "# setup OpenAI model generation\n",
    "with open('GPT_SECRET_KEY_CW.json') as f:\n",
    "    data = json.load(f)\n",
    "openai.api_key = data[\"API_KEY\"]\n",
    "gpt = GPT(engine=\"davinci-instruct-beta-v3\",\n",
    "         temperature=0.9,\n",
    "         max_tokens=500)\n",
    "\n",
    "# calling the pose id model\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# remeber to train a new one in the place\n",
    "# open the working ID model\n",
    "with open('body_language.pkl', 'rb') as f:\n",
    "    poseIDmodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables and functions definition\n",
    "\n",
    "# text_length = 100\n",
    "camera_number = 1\n",
    "\n",
    "happy = [\"Write a happy poem that is a haiku, line endings should not rhyme. The poem should contain once the words \",\n",
    "        \"Write a happy poem that is between six and sixteen lines long. The poem should be written in free verse form and it's line endings should not rhyme. The poem should contain once or twice the words\",\n",
    "        \"Write a happy poem that is a villanelle in the style of Arthur Rimbaud. The poem should contain once the words \",\n",
    "        \"Write a happy poem that is an ode in the style of Virgina Wolf. The poem should contain once the words \",\n",
    "        \"Write a happy poem that is between six and sixteen lines long in the style of Quevedo. The poem should contain once the words \",\n",
    "        \"Write a happy poem that is between six and sixteen lines long in the style of William Shakespear. The poem should contain once the words \",\n",
    "        ]\n",
    "\n",
    "sad = [\"Write a sad poem that is a haiku, line endings should not rhyme. The poem should contain once the words \",\n",
    "        \"Write a sad poem that is between six and sixteen lines long. The poem should be written in free verse form and it's line endings should not rhyme. The poem should contain once or twice the words\",\n",
    "        \"Write a sad poem that is a villanelle in the style of Arthur Rimbaud. The poem should contain once the words \",\n",
    "        \"Write a sad poem that is an ode in the style of Virgina Wolf. The poem should contain once the words \",\n",
    "        \"Write a sad poem that is between six and sixteen lines long in the style of Quevedo. The poem should contain once the words \",\n",
    "        \"Write a sad poem that is between six and sixteen lines long in the style of William Shakespear. The poem should contain once the words \",\n",
    "        ]\n",
    "\n",
    "angry = [\"Write a angry poem that is a haiku, line endings should not rhyme. The poem should contain once the words \",\n",
    "        \"Write a angry poem that is between six and sixteen lines long. The poem should be written in free verse form and it's line endings should not rhyme. The poem should contain once or twice the words\",\n",
    "        \"Write a angry poem that is a villanelle in the style of Arthur Rimbaud. The poem should contain once the words \",\n",
    "        \"Write a angry poem that is an ode in the style of Virgina Wolf. The poem should contain once the words \",\n",
    "        \"Write a angry poem that is between six and sixteen lines long in the style of Quevedo. The poem should contain once the words \",\n",
    "        \"Write a angry poem that is between six and sixteen lines long in the style of William Shakespear. The poem should contain once the words \",\n",
    "        ]\n",
    "\n",
    "silly = [\"Write a silly poem that is a haiku, line endings should not rhyme. The poem should contain once the words \",\n",
    "        \"Write a silly poem that is between six and sixteen lines long. The poem should be written in free verse form and it's line endings should not rhyme. The poem should contain once or twice the words\",\n",
    "        \"Write a silly poem that is a villanelle in the style of Arthur Rimbaud. The poem should contain once the words \",\n",
    "        \"Write a silly poem that is an ode in the style of Virgina Wolf. The poem should contain once the words \",\n",
    "        \"Write a silly poem that is between six and sixteen lines long in the style of Quevedo. The poem should contain once the words \",\n",
    "        \"Write a silly poem that is between six and sixteen lines long in the style of William Shakespear. The poem should contain once the words \",\n",
    "        ]\n",
    "\n",
    "# Functions\n",
    "def feeling_happy(happy, gender, some_object): \n",
    "    print('Generating. Please wait...')\n",
    "    prompt = random.choice(happy) + gender + some_object + '.'\n",
    "    # res = generator(prompt, max_length=text_length, do_sample=True, temperature=0.9)\n",
    "    res = gpt.submit_request(prompt)\n",
    "    global message_to_TD\n",
    "    message_to_TD = (res.choices[0].text)\n",
    "    print(message_to_TD)\n",
    "\n",
    "def feeling_sad(sad, gender, some_object):\n",
    "    print('Generating. Please wait...')\n",
    "    prompt = random.choice(sad) + gender + some_object + '.'\n",
    "    # res = generator(prompt, max_length=text_length, do_sample=True, temperature=0.9)\n",
    "    res = gpt.submit_request(prompt)\n",
    "    global message_to_TD\n",
    "    # message_to_TD = res[0]['generated_text']\n",
    "    message_to_TD = (res.choices[0].text)\n",
    "    print(message_to_TD)\n",
    "    \n",
    "def feeling_silly(silly, gender, some_object):\n",
    "    print('Generating. Please wait...')\n",
    "    prompt = random.choice(silly) + gender + some_object + '.'\n",
    "    # res = generator(prompt, max_length=text_length, do_sample=True, temperature=0.9)\n",
    "    res = gpt.submit_request(prompt)\n",
    "    global message_to_TD\n",
    "    # message_to_TD = (res[0]['generated_text'])  \n",
    "    message_to_TD = (res.choices[0].text)\n",
    "\n",
    "def feeling_angry(angry, gender, some_object):\n",
    "    print('Generating. Please wait...')\n",
    "    prompt = random.choice(angry) + gender + some_object + '.'\n",
    "    res = gpt.submit_request(prompt)\n",
    "    #res = generator(prompt, max_length=text_length, do_sample=True, temperature=0.9)\n",
    "    global message_to_TD\n",
    "    # message_to_TD = (res[0]['generated_text'])\n",
    "    message_to_TD = (res.choices[0].text)\n",
    "    \n",
    "def getFaceBox(net, frame,conf_threshold = 0.75):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn,1.0,(300,300),[104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0,0,i,2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0,0,i,3]* frameWidth)\n",
    "            y1 = int(detections[0,0,i,4]* frameHeight)\n",
    "            x2 = int(detections[0,0,i,5]* frameWidth)\n",
    "            y2 = int(detections[0,0,i,6]* frameHeight)\n",
    "            bboxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn,(x1,y1),(x2,y2),(0,255,0),int(round(frameHeight/150)),8)\n",
    "\n",
    "    return frameOpencvDnn , bboxes\n",
    "    \n",
    "# Define average find function in a list\n",
    "def most_freq(List):\n",
    "    counter = 0\n",
    "    list_obj = List[0]\n",
    "    \n",
    "    for i in List:\n",
    "        curr_freq = List.count(i)\n",
    "        if(curr_freq<counter):\n",
    "            counter = curr_freq\n",
    "            list_obj = i \n",
    "        \n",
    "        return list_obj\n",
    "    \n",
    "# Define the finding person function   \n",
    "def analyze_person():\n",
    "    # Calling the model for person ID\n",
    "    cap = cv2.VideoCapture(camera_number)\n",
    "    font_scale = 3\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    state_completion = 0\n",
    "    results = []\n",
    "    \n",
    "    # Run the model \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        ClassIndex, confidence, bbox = model.detect(frame, confThreshold=0.7)\n",
    "                                                    \n",
    "        ##print(ClassIndex)\n",
    "        if len(ClassIndex) != 0:\n",
    "            for ClassInd, conf, boxes in zip(\n",
    "                ClassIndex.flatten(), confidence.flatten(), bbox\n",
    "            ):\n",
    "                if ClassInd <= 91:\n",
    "                    cv2.rectangle(frame, boxes, (255, 0, 0), 2)\n",
    "\n",
    "                    #label, con = (classLabels[ClassInd - 1], str(confidence * 100)) \n",
    "\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        #label + \": \" + con,\n",
    "                        classLabels[ClassInd - 1],\n",
    "                        (boxes[0] + 10, boxes[1] + 40),\n",
    "                        font,\n",
    "                        fontScale=font_scale,\n",
    "                        color=(0, 255, 0),\n",
    "                        thickness=3,\n",
    "                    )\n",
    "                    if classLabels[ClassInd - 1] == 'person':\n",
    "                        # Send OSC message to TD\n",
    "                        state_completion = 1\n",
    "                        \n",
    "            cv2.imshow(\"Video\", frame)\n",
    "            if state_completion == 1:\n",
    "                break\n",
    "            if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "                            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return state_completion\n",
    "                        \n",
    "# Computer vision analysis function\n",
    "def computer_vision_capture():\n",
    "    cap = cv2.VideoCapture(camera_number)\n",
    "    emotions = ['Happy','Sad','Silly', 'Angry']\n",
    "    objects = ['door']\n",
    "    seq_forward_time = 8\n",
    "    init_time = time.time()\n",
    "\n",
    "    #Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Recolor feed\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Recolor back\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=1,circle_radius=1)\n",
    "                                     )\n",
    "            # Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=2,circle_radius=2)\n",
    "                                     )\n",
    "            # Left hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(80,256,121), thickness=2,circle_radius=2)\n",
    "                                     )\n",
    "            # Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                      mp_drawing.DrawingSpec(color=(245,117,10), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,256,121), thickness=2,circle_radius=2)\n",
    "                                     )\n",
    "            # Export coordinates\n",
    "            try:\n",
    "                # Extract pose landmarks\n",
    "                pose = results.pose_landmarks.landmark\n",
    "                pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "                # Extract face landmarks \n",
    "                face = results.face_landmarks.landmark\n",
    "                face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "                # Concatenate rows\n",
    "                row = pose_row + face_row\n",
    "\n",
    "    #             # Append class name\n",
    "    #             row.insert(0, class_name)\n",
    "\n",
    "                # Make detections\n",
    "                x = pd.DataFrame([row])\n",
    "                body_language_class = poseIDmodel.predict(x)[0]\n",
    "                body_language_prob = poseIDmodel.predict_proba(x)[0]\n",
    "                if body_language_class != 'Winning':\n",
    "                    emotions.append(body_language_class)\n",
    "                # print(body_language_class, body_language_prob)\n",
    "\n",
    "                # Grab ear coordinates\n",
    "                display_coords = tuple(np.multiply(\n",
    "                                    np.array(\n",
    "                                        (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x,\n",
    "                                         results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                                         , [640, 480]).astype(int))\n",
    "\n",
    "                cv2.rectangle(image,\n",
    "                             (display_coords[0], display_coords[1]+5),\n",
    "                             (display_coords[0]+ len(body_language_class)*20, display_coords[1]-30),\n",
    "                             (245, 117, 16), -1)\n",
    "                cv2.putText(image, body_language_class, display_coords, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            cv2.imshow('Raw webcam feed', image)\n",
    "\n",
    "            # close the video when 30 sec have elapsed\n",
    "            current_time = time.time()\n",
    "            if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            break_time = current_time - init_time\n",
    "            print(break_time)\n",
    "            if break_time > seq_forward_time:          \n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(emotions)\n",
    "    print(most_freq(emotions))\n",
    "    \n",
    "    ### INSERT GENDER ID \n",
    "    cap = cv2.VideoCapture(camera_number)\n",
    "    padding = 20\n",
    "    init_time = time.time()\n",
    "    all_genders = []\n",
    "    \n",
    "    while cv2.waitKey(1) < 0:\n",
    "        #read frame\n",
    "        t = time.time()\n",
    "        hasFrame , frame = cap.read()\n",
    "\n",
    "        if not hasFrame:\n",
    "            cv2.waitKey()\n",
    "            break\n",
    "            \n",
    "        #creating a smaller frame for better optimization\n",
    "        small_frame = cv2.resize(frame,(0,0),fx = 0.5,fy = 0.5)\n",
    "\n",
    "        frameFace ,bboxes = getFaceBox(faceNet,small_frame)\n",
    "        if not bboxes:\n",
    "            #print(\"No face Detected, Checking next frame\")\n",
    "            continue\n",
    "        for bbox in bboxes:\n",
    "            face = small_frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),\n",
    "                    max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "            blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "            genderNet.setInput(blob)\n",
    "            genderPreds = genderNet.forward()\n",
    "            gender = genderList[genderPreds[0].argmax()]\n",
    "            #print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "            \n",
    "            all_genders.append(gender)\n",
    "            \n",
    "\n",
    "            label = \"{}\".format(gender)\n",
    "            cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Gender Detection\", frameFace)     \n",
    "                \n",
    "            break_time = t-init_time\n",
    "               \n",
    "            if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "                \n",
    "        if break_time > seq_forward_time:     \n",
    "            break\n",
    "       \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture(camera_number)\n",
    "    font_scale = 3\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    init_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "\n",
    "        ClassIndex, confidence, bbox = model.detect(frame, confThreshold=0.6)\n",
    "\n",
    "        ##print(ClassIndex)\n",
    "        if len(ClassIndex) != 0:\n",
    "            for ClassInd, conf, boxes in zip(\n",
    "                ClassIndex.flatten(), confidence.flatten(), bbox\n",
    "            ):\n",
    "                if ClassInd <= 91:\n",
    "                    cv2.rectangle(frame, boxes, (255, 0, 0), 2)\n",
    "\n",
    "                    #label, con = (classLabels[ClassInd - 1], str(confidence * 100)) \n",
    "\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        #label + \": \" + con,\n",
    "                        classLabels[ClassInd - 1],\n",
    "                        (boxes[0] + 10, boxes[1] + 40),\n",
    "                        font,\n",
    "                        fontScale=font_scale,\n",
    "                        color=(0, 255, 0),\n",
    "                        thickness=3,\n",
    "                    )\n",
    "                    if classLabels[ClassInd - 1] != 'person' and classLabels[ClassInd - 1] != 'remote':\n",
    "                        if classLabels[ClassInd - 1] not in objects :\n",
    "                            objects.append(classLabels[ClassInd - 1])\n",
    "\n",
    "\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "\n",
    "            # close the video when 30 sec have elapsed\n",
    "            current_time = time.time()\n",
    "            if cv2.waitKey(2) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "            break_time = current_time-init_time\n",
    "            if break_time > seq_forward_time:          \n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(objects)\n",
    "    results = [random.choice(emotions), most_freq(all_genders), random.choice(objects)]    \n",
    "    return results\n",
    "\n",
    "# function that gets the mood and calls the correct generation function\n",
    "def mood_selector(results):\n",
    "    print(results[0])\n",
    "    if results[0] == 'Happy':\n",
    "        print('we are on the happy state')\n",
    "        message_to_send = feeling_happy(happy, results[1], results[2])\n",
    "      \n",
    "    \n",
    "    if results[0] == 'Sad':\n",
    "        print('we are on the sad state')\n",
    "        message_to_send = feeling_sad(sad, results[1], results[2])\n",
    "      \n",
    "        \n",
    "    if results[0] == 'Silly':\n",
    "        print('we are on the silly state')\n",
    "        message_to_send = feeling_silly(silly, results[1], results[2])\n",
    "        \n",
    "    if results[0] == 'Angry':\n",
    "        print('we are on the Angry state')\n",
    "        message_to_send = feeling_angry(angry, results[1], results[2])\n",
    "        \n",
    "        \n",
    "# don't think we neeed a server, do we?\n",
    "# Server osc setup\n",
    "if __name__ == \"__main__\":\n",
    "    ip = \"192.168.178.53\"\n",
    "    sendPort = 12000\n",
    "\n",
    "    # Sending OSC to Processing\n",
    "    client = udp_client.SimpleUDPClient(ip, sendPort)\n",
    "    # Sendin printer OSC RaspPis\n",
    "    client_print = udp_client.SimpleUDPClient(\"192.168.178.149\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = 1\n",
    "id_person = 0\n",
    "capture_results = []\n",
    "# send the initial state message\n",
    "client.send_message(\"/state\", state)\n",
    "# main operating function\n",
    "while True:\n",
    "    client.send_message(\"/st\", state)\n",
    "    # call the analyse person\n",
    "    if state == 1:\n",
    "        print(\"Sate 0: find a person in image\")\n",
    "        id_person = analyze_person()\n",
    "        if id_person:\n",
    "            state = 2\n",
    "            client.send_message(\"/st\", state)\n",
    "            id_person = \"0\"\n",
    "        print(state)\n",
    "    elif state == 2:\n",
    "        print(\"State 1: Analyze persons mood, genre and object\")\n",
    "        capture_results = computer_vision_capture()\n",
    "        print('this are the results')\n",
    "        print(capture_results)\n",
    "        state = 3\n",
    "    elif state == 3:\n",
    "        print(\"State 2: calling the generation functions\")\n",
    "        client.send_message(\"/st\", state)\n",
    "        mood_selector(capture_results)\n",
    "        # This send message was neccesary as TD was not reciving only once\n",
    "        # client.send_message(\"/messageIn\", message_to_TD)\n",
    "        print(message_to_TD)\n",
    "        state = 4\n",
    "        client.send_message(\"/st\", state)\n",
    "        # client.send_message(\"/st\", state)\n",
    "    elif state == 4:\n",
    "        print(\"State 3: send message to screen\")\n",
    "        client.send_message(\"/st\", state)\n",
    "        screen_message = message_to_TD.replace('\\n',' ')\n",
    "        screen_message = screen_message.replace('-','')\n",
    "        client.send_message(\"/messageIn\", screen_message)\n",
    "        client.send_message(\"/st\", state)\n",
    "        time.sleep(30)\n",
    "        state = 5\n",
    "    elif state == 5:\n",
    "        print(\"State 4: Sending message to printer\")\n",
    "        client.send_message(\"/state\", state)\n",
    "        message_to_print = message_to_TD.replace('\"', ' ')\n",
    "        message_to_print = message_to_TD.replace('\\n', '')\n",
    "        client_print.send_message(\"/message_to_raspPi\", message_to_print)\n",
    "        time.sleep(45)\n",
    "        state = 1\n",
    "        client.send_message(\"/state\", state)\n",
    "        \n",
    "    if keyboard.is_pressed('q'):\n",
    "        print('You pressed quit key!')\n",
    "        break  # finishing the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.send_message(\"/messageIn\", message)\n",
    "print(message_to_TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_print.send_message(\"/message_to_raspPi\", \"Something\")\n",
    "client.send_message(\"/state\", 4)\n",
    "client_print.send_message(\"/message_to_raspPi\", \"Man me the way to h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
